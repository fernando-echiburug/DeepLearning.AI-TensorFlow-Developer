{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "1. What does flow_from_directory give you on the ImageGenerator  \n",
    "+ The ability to easily load images for training  \n",
    "+ The ability to pick the size of training images  \n",
    "+ The ability to automacally label images based on their directory name  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2. If my Image is sized 150x150, and I pass a 3x3 Convolution over it, what size is the resulting image?  \n",
    "+ 148x148"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3. If my data is sized 150x150, and I use Pooling of size 2x2, what size will the resulting image be?  \n",
    "+ 75x75"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "4. If I want to view the history of my training, how can I access it?  \n",
    "+ Create a variable 'history' and assign it to the return of model.fit or model.fit_generator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "5. What's the name of the API that allows you to inspect the impact of convolutions on the images?  \n",
    "+ The model.layers API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "6. When exploring the graphs, the loss levelled out at about 0.75 after 2 epochs, but the accuracy climbed close to 1.0 after 15 epochs. What's the significance of this?  \n",
    "+ There was no point training after 2 epochs, as we overfit to the training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "7. Why is the validation accuracy a better indicator of model performance than training accuracy?  \n",
    "+ The validation accuracy is based on images that the model hasn't been trained with, and thus a better indicator of how the model will perform with new images."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "8. Why is overfitting more likely to occur on smaller datasets?  \n",
    "+ Because there's less likelihood of all possible features being encountered in the training process. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}